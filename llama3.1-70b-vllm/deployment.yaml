apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: llama3.1-70b-instruct-anyuid-scc
rules:
  - verbs:
      - use
    resources:
      - securitycontextconstraints
    resourceNames:
      - anyuid
#      Any scc (even custom one ) with allowIPC: true will be appropriate
      - nvidia-mig-manager
    apiGroups:
      - security.openshift.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: llm-70b
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: llama3.1-70b-instruct-anyuid-scc
subjects:
  - kind: ServiceAccount
    name: llm-70b
roleRef:
  kind: Role
  name: llama3.1-70b-instruct-anyuid-scc
  apiGroup: rbac.authorization.k8s.io

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: llama3.1-70b-instruct
    inference: vllm
  name: llama3.1-70b-instruct
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      app: llama3.1-70b-instruct
  template:
    metadata:
      labels:
        app: llama3.1-70b-instruct
    spec:
      hostIPC: true
      volumes:
        - name: llm-cache
          persistentVolumeClaim:
            claimName: llm-cache
      tolerations:
        - effect: NoSchedule
          key: p4-gpu
          operator: Exists
      securityContext:
        runAsUser: 0
      serviceAccountName: llm-70b
      containers:
      - image: docker.io/vllm/vllm-openai:latest
        name: vllm-openai
#        env:
#          - name: CUDA_VISIBLE_DEVICES
#            value: "1"
        workingDir: /vllm-workspace
        command: [ "python3" , "-m" , "vllm.entrypoints.openai.api_server" ]
        args: [ "--model", "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4","--tensor-parallel-size", "2", "--max-model-len", "8192", "--max_num_seqs", "128" ]
        volumeMounts:
          - mountPath: /root/.cache/huggingface
            name: llm-cache
        resources:
          limits:
            cpu: 1000m
            memory: 35Gi
            nvidia.com/gpu: "2"
          requests:
            cpu: 500m
            memory: 25Gi
            nvidia.com/gpu: "2"
---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llm-cache
  labels:
    app: llama3.1-70b-instruct
    inference: vllm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: gp3-csi

---

apiVersion: v1
kind: Service
metadata:
  labels:
    app: llama3.1-70b-instruct
    inference: vllm
  name: llama3-1-70b-instruct
spec:
  ports:
    - port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app: llama3.1-70b-instruct

---

apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: "true"
  labels:
    app: llama3.1-70b-instruct
    inference: vllm
  name: llama3-1-70b-instruct
spec:
  port:
    targetPort: 8000
  to:
    kind: Service
    name: llama3-1-70b-instruct
    weight: 100